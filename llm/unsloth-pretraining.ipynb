{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a169de6-8258-4af0-8b29-39b9b8280cf1",
   "metadata": {},
   "source": [
    "# Pre-training example using Unsloth \n",
    "This is a very basic example of training llama3.2 using unsloth. It will train on plain text so only continued pre-traning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fdd9f3-660e-48de-ace7-fa2d0a0b1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ccd7b22-6355-4cce-b013-c3fd34362fb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in ./jupyter_env/lib/python3.12/site-packages (2024.12.12)\n",
      "Found existing installation: unsloth 2024.12.12\n",
      "Uninstalling unsloth-2024.12.12:\n",
      "  Successfully uninstalled unsloth-2024.12.12\n",
      "Collecting git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to /tmp/pip-req-build-o12zjxia\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git /tmp/pip-req-build-o12zjxia\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 87f5bffc45a8af7f23a41650b30858e097b86418\n",
      "  Installing build dependencies ... \u001b[?2done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for unsloth: filename=unsloth-2024.12.12-py3-none-any.whl size=175166 sha256=25bef00eecf2b779e977f2064969cfac03e12758fede0f086487cc0ba60554e8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-dm21wyr9/wheels/60/3e/1f/e576c07051d90cf64b6a41434d87ccf4db33fafd5343bf5de0\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "Successfully installed unsloth-2024.12.12\n"
     ]
    }
   ],
   "source": [
    "# Install unsloth\n",
    "!pip install unsloth\n",
    "# Also get the latest nightly Unsloth!\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23f63cec-366a-4ca7-b6e4-5bdd2dff2e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in ./jupyter_env/lib/python3.12/site-packages (0.19.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in ./jupyter_env/lib/python3.12/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in ./jupyter_env/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./jupyter_env/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in ./jupyter_env/lib/python3.12/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in ./jupyter_env/lib/python3.12/site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in ./jupyter_env/lib/python3.12/site-packages (from wandb) (6.1.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in ./jupyter_env/lib/python3.12/site-packages (from wandb) (2.10.4)\n",
      "Requirement already satisfied: pyyaml in ./jupyter_env/lib/python3.12/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in ./jupyter_env/lib/python3.12/site-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in ./jupyter_env/lib/python3.12/site-packages (from wandb) (2.19.2)\n",
      "Requirement already satisfied: setproctitle in ./jupyter_env/lib/python3.12/site-packages (from wandb) (1.3.4)\n",
      "Requirement already satisfied: setuptools in ./jupyter_env/lib/python3.12/site-packages (from wandb) (75.6.0)\n",
      "Requirement already satisfied: six>=1.4.0 in ./jupyter_env/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in ./jupyter_env/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./jupyter_env/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./jupyter_env/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./jupyter_env/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./jupyter_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./jupyter_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./jupyter_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./jupyter_env/lib/python3.12/site-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in ./jupyter_env/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Install WandB (weight and biases) for nice graphs and result tracking\n",
    "!pip install wandb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbef3832-169c-4ccc-9598-fe69b8a1edb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoakim_eriksson\u001b[0m (\u001b[33mjoakim_eriksson-rise-research-institutes-of-sweden\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Login to Wandb\n",
    "!wandb login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d99bd8-1250-4339-aec2-2b9bf84361e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjoakim_eriksson\u001b[0m (\u001b[33mjoakim_eriksson-rise-research-institutes-of-sweden\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/wandb/run-20250103_152007-aeowz0cf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/joakim_eriksson-rise-research-institutes-of-sweden/contiki-llama-pretraining/runs/aeowz0cf' target=\"_blank\">llama-3.2-contiki-run</a></strong> to <a href='https://wandb.ai/joakim_eriksson-rise-research-institutes-of-sweden/contiki-llama-pretraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/joakim_eriksson-rise-research-institutes-of-sweden/contiki-llama-pretraining' target=\"_blank\">https://wandb.ai/joakim_eriksson-rise-research-institutes-of-sweden/contiki-llama-pretraining</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/joakim_eriksson-rise-research-institutes-of-sweden/contiki-llama-pretraining/runs/aeowz0cf' target=\"_blank\">https://wandb.ai/joakim_eriksson-rise-research-institutes-of-sweden/contiki-llama-pretraining/runs/aeowz0cf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/joakim_eriksson-rise-research-institutes-of-sweden/contiki-llama-pretraining/runs/aeowz0cf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f8b45adfef0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# Initialize W&B\n",
    "wandb.init(\n",
    "    project=\"contiki-llama-pretraining\",\n",
    "    name=\"llama-3.2-contiki-run\",\n",
    "    config={\n",
    "        \"model\": \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\",\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"batch_size\": 4,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"max_steps\": 500\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79b1258d-0aa7-4438-bbca-858f2b0a822b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file dataset/pdf_pretraining_dataset.json\n",
      "  found 106 entries.\n",
      "Loading file dataset/pretraining_dataset.json\n",
      "  found 2811 entries.\n",
      "Loaded 2917 examples from datasets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2887\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 30\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load all datasets from the dataset directory\n",
    "dataset_dir = Path('dataset')\n",
    "allowed_types = ('repository_file', 'pdf_text')\n",
    "all_data = []\n",
    "for file in dataset_dir.glob('*.json'):\n",
    "    print(f\"Loading file {file}\")\n",
    "    data = []\n",
    "    with open(file, 'r', encoding='utf-8') as f:  # Open the file\n",
    "        jsdata = json.load(f)\n",
    "        for entry in jsdata:\n",
    "            if entry['type'] in allowed_types:\n",
    "                data.append({'text' : entry['output']})\n",
    "        print(f\"  found {len(data)} entries.\")\n",
    "        all_data.extend(data)\n",
    "        \n",
    "print(f\"Loaded {len(all_data)} examples from datasets\")\n",
    "wandb.log({\"dataset_size\": len(all_data)})\n",
    "\n",
    "# Convert to HuggingFace dataset\n",
    "dataset = Dataset.from_list(all_data)\n",
    "dataset.train_test_split(test_size = 0.01)\n",
    "#print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7732bf9e-d96b-4ccc-ac30-6fdd96643538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load unsloth/Llama-3.2-3B-Instruct-bnb-4bit...\n",
      "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 24.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Successfully loaded model: unsloth/Llama-3.2-3B-Instruct-bnb-4bit\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments\n",
    "# Initialize model and tokenizer\n",
    "model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "\n",
    "# Alternative models if needed\n",
    "backup_models = [\n",
    "    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "]\n",
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "def try_load_model(model_names):\n",
    "    for name in model_names:\n",
    "        try:\n",
    "            print(f\"Attempting to load {name}...\")\n",
    "            # Load the tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(name, trust_remote_code=True)\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "            # Load the model with unsloth optimizations\n",
    "            model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "                model_name=name,\n",
    "                max_seq_length=2048,\n",
    "                dtype=None,\n",
    "                load_in_4bit=True,\n",
    "            )\n",
    "            print(f\"Successfully loaded model: {name}\")\n",
    "            wandb.log({\"model_loaded\": name})\n",
    "            return model, tokenizer\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {name}: {str(e)}\")\n",
    "            wandb.log({\"model_load_error\": {\"model\": name, \"error\": str(e)}})\n",
    "    raise Exception(\"Failed to load any model\")\n",
    "\n",
    "# Try to load models in order of preference\n",
    "model, tokenizer = try_load_model([model_name] + backup_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec15679-b2f9-4564-ba69-68d960aff98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading input_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/jupyter_env/lib/python3.12/site-packages/unsloth/models/_utils.py:747: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  offloaded_W = torch.load(filename, map_location = \"cpu\", mmap = True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Offloading output_embeddings to disk to save VRAM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.12 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Training embed_tokens in mixed precision to save VRAM\n",
      "Unsloth: Training lm_head in mixed precision to save VRAM\n"
     ]
    }
   ],
   "source": [
    "# From Unslot pretraining example\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 128, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "\n",
    "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = True,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76481de1-783f-471f-b1b9-1cf012fa9d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████| 2917/2917 [00:00<00:00, 69927.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    return { \"text\" : [example + EOS_TOKEN for example in examples[\"text\"]] }\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7dcf36e0-2f4f-4f2c-9a81-b8fefdda2de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "Digital Comprehensive Summaries of Uppsala Dissertations\n",
      "\n",
      "from the Faculty of Science and Technology 2335\n",
      "\n",
      "Scalable and Interoperable Low-Power\n",
      "\n",
      "Internet of Things Networks\n",
      "\n",
      "JOAKIM ERIKSSON\n",
      "\n",
      "ACTA UNIVERSITATIS\n",
      "\n",
      "UPSALIENSIS\n",
      "\n",
      "2023\n",
      "\n",
      "ISSN 1651-6214\n",
      "\n",
      "ISBN 978-91-513-1951-3\n",
      "\n",
      "urn:nbn:se:uu:diva-513926<|eot_id|>\n",
      "=========================\n",
      "Dissertation presented at Uppsala University to be publicly examined in Häggsalen,\n",
      "\n",
      "Ångströmlaboratoriet, Lägerhyddsvägen 1, Uppsala, Friday, 15 December 2023 at 13:15 for\n",
      "\n",
      "the degree of Doctor of Philosophy. The examination will be conducted in English. Faculty\n",
      "\n",
      "examiner: Professor Leo Selavo (University of Latvia ).\n",
      "\n",
      "Abstract\n",
      "\n",
      "Eriksson, J. 2023. Scalable and Interoperable Low-Power Internet of Things Networks.\n",
      "\n",
      "Digital Comprehensive Summaries of Uppsala Dissertations from the Faculty of Science and\n",
      "\n",
      "Technology 2335. 48 pp. Uppsala: Acta Universitatis Upsaliensis. ISBN 978-91-513-1951-3.\n",
      "\n",
      "Internet of Things (IoT) is the concept of connecting devices to the Internet. IoT devices can be\n",
      "\n",
      "anything from small temperature sensors to self-driving cars. The devices are typically resource-\n",
      "\n",
      "constrained, connected wirelessly, and often battery-powered. In this thesis, we address energy\n",
      "\n",
      "efficiency and the tools required for estimating power consumption, interoperability between\n",
      "\n",
      "different implementations of IoT protocols, and scalability of the IoT networks in mesh\n",
      "\n",
      "configurations. The contributions are made in the five included research papers addressing\n",
      "\n",
      "these challenges. Firstly, we present and evaluate network-wide energy estimation support\n",
      "\n",
      "in our simulation tool COOJA/MSPSim. Due to the timing accuracy of the simulation and\n",
      "\n",
      "emulation, we get energy consumption estimates very close to hardware-based estimates. The\n",
      "\n",
      "second contribution evaluates the capabilities of simulation tools for interoperability testing.\n",
      "\n",
      "We show that it is possible to set up simulations of networks with multiple implementations\n",
      "\n",
      "of the same open standards (6LoWPAN/RPL) and that it is possible to get results beyond pure\n",
      "\n",
      "interoperability, including power consumption and network quality. Finally, we show that, by\n",
      "\n",
      "carefully managing neighbor updates, it is possible to scale IoT networks even when the IoT\n",
      "\n",
      "devices' memory limitations severely constrain the size of the neighbor table.<|eot_id|>\n",
      "=========================\n",
      "The experimental systems research that resulted in this thesis also provided significant\n",
      "\n",
      "contributions to the open-source ecosystem around Contiki, an operating system for resource-\n",
      "\n",
      "constrained IoT devices. This software, Contiki and COOJA/MSPSim, has been a cornerstone\n",
      "\n",
      "in our capability to perform sound systems research and has been widely used by other research\n",
      "\n",
      "groups in resource-constrained IoT research in academia and many companies for developing\n",
      "\n",
      "commercial IoT devices.\n",
      "\n",
      "Keywords: IoT, low-power networking, scalability, interoperability\n",
      "\n",
      "Joakim Eriksson, Department of Electrical Engineering, Networked Embedded Systems, Box\n",
      "\n",
      "65, Uppsala University, SE-751 03 Uppsala, Sweden.\n",
      "\n",
      "© Joakim Eriksson 2023\n",
      "\n",
      "ISSN 1651-6214\n",
      "\n",
      "ISBN 978-91-513-1951-3\n",
      "\n",
      "URN urn:nbn:se:uu:diva-513926 (http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-513926)<|eot_id|>\n",
      "=========================\n",
      "List of papers\n",
      "\n",
      "This thesis is based on the following papers, which are referred to in the text\n",
      "\n",
      "by their Roman numerals.\n",
      "\n",
      "I\n",
      "\n",
      "Joakim Eriksson, Fredrik Österlind, Niclas Finne, Adam Dunkels,\n",
      "\n",
      "Nicolas Tsiftes, and Thiemo Voigt, \"Accurate, network-scale power\n",
      "\n",
      "profiling for sensor network simulators\". In European Conference on\n",
      "\n",
      "Wireless Sensor Networks, EWSN 2009 DOI:\n",
      "\n",
      "https://doi.org/10.1007/978-3-642-00224-3_20\n",
      "\n",
      "II\n",
      "\n",
      "Joakim Eriksson, Fredrik Österlind, Niclas Finne, Nicolas Tsiftes,\n",
      "\n",
      "Adam Dunkels, Thiemo Voigt, Robert Sauter, and Pedro José Marrón,\n",
      "\n",
      "\"COOJA/MSPSim: Interoperability Testing for Wireless Sensor\n",
      "\n",
      "Networks\". In International Conference on Simulation Tools and\n",
      "\n",
      "Techniques, Simutools 2009 DOI:\n",
      "\n",
      "http://dx.doi.org/10.4108/ICST.SIMUTOOLS2009.5637\n",
      "\n",
      "III\n",
      "\n",
      "JeongGil Ko, Joakim Eriksson, Nicolas Tsiftes, Stephen\n",
      "\n",
      "Dawson-Haggerty, Jean-Philippe Vasseur, Mathilde Durvy, Andreas\n",
      "\n",
      "Terzis, Adam Dunkels, and David Culler, \"Industry: Beyond\n",
      "\n",
      "Interoperability - Pushing the Performance of Sensor Network IP\n",
      "\n",
      "Stacks\". In ACM Conference on Embedded Networked Sensor Systems,\n",
      "\n",
      "SenSys 2011 DOI: https://doi.org/10.1145/2070942.2070944\n",
      "\n",
      "IV\n",
      "\n",
      "George Oikonomou, Simon Duquennoy, Atis Elsts, Joakim Eriksson,\n",
      "\n",
      "Yasuyuki Tanaka, and Nicolas Tsiftes, \"The Contiki-NG open source\n",
      "\n",
      "operating system for next generation IoT devices\". In SoftwareX,\n",
      "\n",
      "Volume 18, June 2022 DOI:\n",
      "\n",
      "https://doi.org/10.1016/j.softx.2022.101089\n",
      "\n",
      "V\n",
      "\n",
      "Joakim Eriksson, Niclas Finne, Nicolas Tsiftes, Simon Duquennoy,\n",
      "\n",
      "and Thiemo Voigt, \"Scaling RPL to Dense and Large Networks with\n",
      "\n",
      "Constrained Memory\". In International Conference on Embedded\n",
      "\n",
      "Systems and Networks, EWSN 2018 DOI:\n",
      "\n",
      "https://dl.acm.org/doi/10.5555/3234847.3234863\n",
      "\n",
      "Reprints were made with permission from the publishers.<|eot_id|>\n",
      "=========================\n",
      "Additional Publications\n",
      "\n",
      "List of selected additional publications not included in the Thesis:\n",
      "\n",
      "• Niclas Finne, Joakim Eriksson, Thiemo Voigt, George Suciu, Mari-Anais\n",
      "\n",
      "Sachian, JeongGil Ko, and Hossein Keipour, \"Multi-trace: multi-level\n",
      "\n",
      "data trace generation with the COOJA simulator\". In Intelligent Systems\n",
      "\n",
      "for the Internet of Things, Workshop at 17th International Conference\n",
      "\n",
      "on Distributed Computing in Sensor Systems (DCOSS), 2021\n",
      "\n",
      "• Shuai Zhu, Thiemo Voigt, Daniel F Perez-Ramirez, and Joakim Eriks-\n",
      "\n",
      "son \"A Low-resolution infrared thermal dataset and potential privacy-\n",
      "\n",
      "preserving applications\". In DATA’21, Workshop at 19th ACM Confer-\n",
      "\n",
      "ence on Embedded Networked Sensor Systems (SenSys), 2021\n",
      "\n",
      "• John Kanwar, Niclas Finne, Nicolas Tsiftes, Joakim Eriksson, Thiemo\n",
      "\n",
      "Voigt, Zhitao He, Christer Åhlund, and Saguna Saguna \"JamSense: In-\n",
      "\n",
      "terference and Jamming Classification for Low-power Wireless Networks\".\n",
      "\n",
      "In 13th IFIP Wireless and Mobile Networking Conference (WMNC),\n",
      "\n",
      "2021\n",
      "\n",
      "• Eric Samikwa, Thiemo Voigt, and Joakim Eriksson, \"Flood prediction\n",
      "\n",
      "using IoT and artificial neural networks with edge computing\". In Inter-\n",
      "\n",
      "national Conferences on Internet of Things (iThings), 2020\n",
      "\n",
      "• Carlos Gonzalo Peces, Joakim Eriksson, and Nicolas Tsiftes, \"Sleepy\n",
      "\n",
      "Devices Versus Radio Duty Cycling: The Case of Lightweight M2M\".\n",
      "\n",
      "In IEEE Internet of Things Journal nr 2, volume 6, 2019\n",
      "\n",
      "• Luca Mottola, Gian Pietro Picco, Felix Jonathan Oppermann, Joakim\n",
      "\n",
      "Eriksson, Niclas Finne, Harald Fuchs, Andrea Gaglione et al. \"make-\n",
      "\n",
      "Sense: Simplifying the Integration of Wireless Sensor Networks into\n",
      "\n",
      "Business Processes\". In IEEE Transactions on Software Engineering\n",
      "\n",
      "45, 2017\n",
      "\n",
      "• Simon Duquennoy, Joakim Eriksson and Thiemo Voigt, \"Five-Nines Re-\n",
      "\n",
      "liable Downward Routing in RPL\". In arXiv 2017\n",
      "\n",
      "• Oriol Piñol Piñol, Shahid Raza, Joakim Eriksson, and Thiemo Voigt,\n",
      "\n",
      "\"BSD-based elliptic curve cryptography for the open Internet of Things\".<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "# Print out some of the data\n",
    "for row in dataset[:5][\"text\"]:\n",
    "    print(\"=========================\")\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e263e3-a676-4530-830e-8bf29b6c78bf",
   "metadata": {},
   "source": [
    "# Setup the Unsloth Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "667c9ce9-8fe3-4815-880b-26252423c36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=8): 100%|████████████████████████████████████████████████████████████████████████████| 2917/2917 [00:08<00:00, 332.71 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import UnslothTrainer, UnslothTrainingArguments\n",
    "\n",
    "trainer = UnslothTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 8,\n",
    "\n",
    "    args = UnslothTrainingArguments(\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 8,\n",
    "\n",
    "        warmup_ratio = 0.1,\n",
    "        num_train_epochs = 1,\n",
    "\n",
    "        learning_rate = 5e-5,\n",
    "        embedding_learning_rate = 5e-6,\n",
    "\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.00,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"llama-3.2-contiki-pretrain-run\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8250db6b-54ae-448c-96ba-bedc7a0e83a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3090. Max memory = 24.0 GB.\n",
      "6.578 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e71b58ac-475b-4777-ab4d-2e8b70c2fc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 2,917 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 8\n",
      "\\        /    Total batch size = 32 | Total steps = 91\n",
      " \"-____-\"     Number of trainable parameters = 982,515,712\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='91' max='91' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [91/91 36:17, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.868900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.807600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.838600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.770300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.899400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.738300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.735100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.666400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.759900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.789900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.790500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.614900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.673800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.729800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.577300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.740300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.650500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.840800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.718400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.836700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.735100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.777000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.732900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.710300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.573200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.641900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.759000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.648700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.679800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.626400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.679100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.588500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.700200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.679100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.559300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.503100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.760700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.634700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.664800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.628000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.537900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.696700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.706700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.541600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.589600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.652000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.583100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.462400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.656700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.629700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.465500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.556900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.770500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.541300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.566400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.508800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.769500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.643900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.608600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.584700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.597200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.738300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.546200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.627400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.719600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.703600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.583400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.677800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.585200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.683000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.573900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.623900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.599900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.705700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.631000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.791200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.738100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.574400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28368036-60fb-44f0-8eaa-d32fe9b9f9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2206.4137 seconds used for training.\n",
      "36.77 minutes used for training.\n",
      "Peak reserved memory = 12.727 GB.\n",
      "Peak reserved memory for training = 6.149 GB.\n",
      "Peak reserved memory % of max memory = 53.029 %.\n",
      "Peak reserved memory for training % of max memory = 25.621 %.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#@title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "021a23b9-58d3-4cef-97ca-365270768d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and log to W&B\n",
    "trainer.save_model(\"contiki_llama32_pretrain_model_final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bfc834-9a5b-46bb-908c-19bc798dd614",
   "metadata": {},
   "source": [
    "# Test inferencing of the model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "89b25944-b5bd-4ce4-ba85-aa72753a285b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.12: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 3090. Max memory: 24.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Select the model by setting which model to use...\n",
    "if False:\n",
    "    model = trainer.model\n",
    "elif False:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"contiki_llama32_pretrain_model_final\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "else:\n",
    "    from unsloth import FastLanguageModel\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length = 2048,\n",
    "        dtype = None,\n",
    "        load_in_4bit = True,\n",
    "    )\n",
    "\n",
    "inference_model = FastLanguageModel.for_inference(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "11196a7a-a007-4293-97e2-5ee41a6152e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|>The contributors of Contiki-NG are Theodoros Kasapidis\n",
      " Antonios G. \n",
      "Papadopoulos\n",
      "          Georgios P. Papadopoulos\n",
      "M. Papadopoulosantinos \n",
      "Vasilios K. \n",
      "Papadopoulos\n",
      " Andreas K. Papadopoulos\n",
      "          Georgios D. Papadopoulos\n",
      "P.        Georgios \n",
      "Papadopoulos\n",
      "Vasilios K. Papadopoulos\n",
      "Andreas K. Papadopoulos\n",
      " Georgios D. \n",
      "Papadopoulos\n",
      "          Georgios P. Papadopoulos\n",
      "K. Papadopoulosios \n",
      "K.        Andreas \n",
      "Papadopoulos\n",
      "Georgios D. Papadopoulos\n",
      " Georgios P. Papadopoulos\n",
      "K.        Vasilios \n",
      "Papadopoulos\n",
      "Andreas K. Papadopoulos\n",
      "          Georgios D. Papadopoulos\n",
      "P.        Georgios \n",
      "Papadopoulos\n",
      "Vasilios K. Papadopoulos\n",
      "          Andreas K. Papadopoulos\n",
      "Georgios D. \n",
      "Papadopoulos\n",
      "Papadopoulosorgios P. \n",
      "Vasilios K. Papadopoulos\n",
      "          Andreas K. \n",
      "Papadopoulos\n",
      "          Georgios D. Papadopoulos\n",
      "Papadopoulosorgios P. \n",
      "Vasilios K"
     ]
    }
   ],
   "source": [
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "text_streamer = TextIteratorStreamer(tokenizer)\n",
    "import textwrap\n",
    "max_print_width = 100\n",
    "\n",
    "# Fill in style\n",
    "if True:\n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "        \"\"\"The contributors of Contiki-NG are\n",
    "         \"\"\"\n",
    "    ]*1, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# Instruction style\n",
    "else: \n",
    "    inputs = tokenizer(\n",
    "    [\n",
    "    \"\"\" Based on given instruction and context, generate an appropriate response. You are a code co-pilot helping out answering on Contiki-NG related questions.\n",
    "### Instruction:\n",
    "Write a hello world program in Contiki-NG style - with a process and an e-timer.\n",
    "### Context:\n",
    "Show a code snippet with good comments.\n",
    "### Response:\n",
    "\"\"\"\n",
    "    ]*1, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "\n",
    "generation_kwargs = dict(\n",
    "    inputs,\n",
    "    streamer = text_streamer,\n",
    "    max_new_tokens = 256,\n",
    "    use_cache = True,\n",
    ")\n",
    "thread = Thread(target = inference_model.generate, kwargs = generation_kwargs)\n",
    "thread.start()\n",
    "\n",
    "length = 0\n",
    "for j, new_text in enumerate(text_streamer):\n",
    "    if j == 0:\n",
    "        wrapped_text = textwrap.wrap(new_text, width = max_print_width)\n",
    "        length = len(wrapped_text[-1])\n",
    "        wrapped_text = \"\\n\".join(wrapped_text)\n",
    "        print(wrapped_text, end = \"\")\n",
    "    else:\n",
    "        length += len(new_text)\n",
    "        if length >= max_print_width:\n",
    "            length = 0\n",
    "            print()\n",
    "        print(new_text, end = \"\")\n",
    "    pass\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916bae7a-1b58-45b5-9969-ca13d492a620",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca3b3fe-f1a2-48b1-94b5-51c0e830b2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
